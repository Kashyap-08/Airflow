{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Tutorial: https://www.youtube.com/watch?v=K9AnJ9_ZAXE&t=5484s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operators\n",
    "\n",
    "* There are multiple different operators in Airflow.\n",
    "* Operaror defins the work that the task needes to do.\n",
    "* There are many operators, some of them are:\n",
    "    * PythonOperator: Use to run python task.\n",
    "    * BashOperator: These are used to perform simple operations like running a Python function, executing a Bash script, or interacting with the system.\n",
    "    * DummyOperator: A placeholder operator that does nothing.\n",
    "    * CustomOperator: Can create a custom operator as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Lifecycle\n",
    "\n",
    "![alt text](Images/task_lifecycle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baisc Architecture\n",
    "\n",
    "![alt text](Images/basic_architecture.png)\n",
    "\n",
    "\n",
    "* `Dag`: A DAG is the core concept in Airflow, representing a collection of tasks with a defined execution order. DAGs define the workflows but do not execute them.\n",
    "* `Scheduler`: The scheduler is responsible for reading DAGs, determining their execution schedules, and deciding when to run tasks. It monitors time intervals, external triggers, and DAG dependencies.\n",
    "* `Executors`: The executor is responsible for executing the tasks that the scheduler has scheduled.\n",
    "* `Workers`: Workers are responsible for executing the tasks assigned to them by the scheduler via the executor.\n",
    "* `Web Server`: The web server provides a user interface to monitor, manage, and trigger workflows.\n",
    "* `Metadata Database`: The metadata database stores all information about DAGs, tasks, users, and their states.\n",
    "* `Logs`: Logs are an essential part of monitoring task execution.\n",
    "* `Airflow.cfg`: The airflow.cfg file is the main configuration file for Apache Airflow. It contains settings that define how Airflow operates, including its database connection, executors, logging, scheduling, and security options. Each section in the configuration file corresponds to a specific part of the Airflow ecosystem, and tweaking these settings allows you to fine-tune how Airflow runs in your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow Xcom\n",
    "\n",
    "* you can push or pull data to or from airflow using xcom\n",
    "* maximum allowed data sharing using Xcom is `48KB`.\n",
    "* Never use Xcom to share large data. i.e. DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow Decorators\n",
    "\n",
    "* Airflow decorators are a more Pythonic and concise way to define DAGs, tasks, and other configurations in Apache Airflow. \n",
    "* Instead of explicitly creating and configuring DAGs and operators using traditional code, decorators allow you to annotate Python functions to automatically handle this.\n",
    "* Introduced in Airflow 2.0, these decorators simplify writing DAGs by allowing you to define DAGs and tasks more declaratively, improving code readability and organization.\n",
    "* Common Airflow Decorators:\n",
    "    * @dag: Used to define a DAG directly from a Python function.\n",
    "    * @task: Used to define individual tasks (like PythonOperator) as functions.\n",
    "    * @task_group: Used to define a group of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dag Catchup and BackFill\n",
    "\n",
    "* Airflow Provides the option to Catchup the dag runs from the start date.\n",
    "    * assume, you created dag on 2024-10-02, but you want the data from 2024-09-28, in this case you can set the `catchup = True` in DAG defination.\n",
    "* Airflow also provides the option to Backfill missig days data using the terminal.\n",
    "    * for this you need to get the container list using command `docker ps`.\n",
    "    * get the airflow-scheduler container_id.\n",
    "    * loggin into the container using command `docker exec -it <CONTAINER_ID> bash`.\n",
    "    * once you logged in as a user, then hit command: `airflow dags backfill -s <Date from when you want bacfill> -e <Date till when you want backfill> <DAG_ID>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedule_Interval\n",
    "\n",
    "* YOu can schedule the dag using `schedule_interval`.\n",
    "* There are 2 ways of Scheduling dag:\n",
    "    * datetime.timedelta\n",
    "    * cron Expression:\n",
    "        * Its a string comprise of five fields seperated by white space that represent the set of time.\n",
    "        * Eg: 15 14 1 * * (`minute`     `hour`      `day_of_month`      `month`     `day_of_week`)\n",
    "        * Airflow provides some presets:\n",
    "        * ![alt text](Images/schedule_interval_preset.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow Connections\n",
    "\n",
    "* YOu need to connnect to the external resoueces while creating DAgs.\n",
    "* Airflow provides option to connect to different external resources such as (Databases, Cloud Servers, Others)\n",
    "* To use the prostre DB connector, update docer-compose.yaml file with `ports: 5432:5432`.\n",
    "* Recreate Postgre container using command: \n",
    "    * `docker-compose up -d --no-deps --build postgres` \n",
    "    * OR \n",
    "    * `docker-compose up -d --no-deps --build postgres airflow_docker_postgres_1  is up_to_date`\n",
    "* Download Dbeaver Application and create a new connection within it with port 5432.\n",
    "* Once done, add the connection to the airflow by going through the `Admin` > `Connection`\n",
    "* Now you can insert or delete new data to the database using airflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow Docker Install Python Package\n",
    "\n",
    "* There are 2 ways to install python dependencies to your airflow docker container.\n",
    "    * Image Extending\n",
    "    * Image Customising\n",
    "\n",
    "![alt text](Images/install_python_package_in_container.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending a Docker Image\n",
    "\n",
    "* TO install new libraries in the existing docker image, use the below steps:\n",
    "    * create requirements.txt file and provide all the libraries you want to install.\n",
    "    * create a file named `Dockerfile` and write the command:\n",
    "\n",
    "                FROM apache/airflow:2.5.1           # Define the image you want to extend\n",
    "                COPY requirements.txt /requirements.txt # Copy the created requiremtn file to docker image\n",
    "                RUN pip install --user --upgrade pip    # Upgrade pip \n",
    "                RUN pip install --no-cache-dir --user -r /requirements.txt  # Install libraries form requirements file.\n",
    "\n",
    "    * ONce file is created, hit command: `docker build . --tag extending_airflow:latest`\n",
    "    * ONce all the packages are installed, get into the `docker-compose.yaml` file, and change the image name from\n",
    "        * `image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.5.1}`\n",
    "        * to\n",
    "        * `image: ${AIRFLOW_IMAGE_NAME:-extending_airflow:latest}`\n",
    "        >> NOte: The changed image name should be the same as tag name, while building docker.\n",
    "    * Test out the installed packages.\n",
    "\n",
    "* YOu need to rebuild the airflow webserver and scheduler services as we modified the airflow image name in the docker-compose.yml file.\n",
    "* To do so, hit below commands:\n",
    "    * `docker-compose up -d --no-deps --build airflow-webserver airflow-scheduler`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize Docker Image\n",
    "\n",
    "* Clone the airflow git repository: https://github.com/apache/airflow.git\n",
    "* Look for `docker-context-files`\n",
    "* create new requirements.txt file within it.\n",
    "* Build Docker image from scrath by using command: `docker build . --build-arg AIRFLOW_VERSION='2.0.1' --tag customising_airflow:latest`\n",
    "* ONce Build is completed, get into the docker-compose.yml file and update the image name to \n",
    "    * `image: ${AIRFLOW_IMAGE_NAME:-customising_airflow:latest}`\n",
    ">> NOTE: the tag name and the image name should be the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHen to Extend and Customie Docker Image\n",
    "\n",
    "| Image Extending | Image Customising |\n",
    "|---|---|\n",
    "| Can go with it 99% of the time. | If you want more things to customise|\n",
    "| Easy to Use | Care about Image size Optimization |\n",
    "| Build Fast ||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow AWS S3 SEnsor\n",
    "\n",
    "* A Sensor is the special type of operator which waits for something to occur.\n",
    "* Use case: When you dont know when the dag should be triggered.\n",
    "* To test S3 sensor we are going to use `MinIO`.\n",
    "* MinIO is an open-source, high-performance object storage system, that resembels AWS S3.\n",
    "* Use command to run MinIO container: `docker run -p 9000:9000 -p 9001:9001 -e \"MINIO_ROOT_USER=AKIAIOSFODNN7EXAMPLE\" -e \"MINIO_ROOT_PASSWORD=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" quay.io/minio/minio server /data --console-address \":9001\"`\n",
    "* Get into the webUP and login using the userid password provided in above command.\n",
    "* Now look for the `amazon-provider` version by following the below steps:\n",
    "    * Get list of airflow services running: `docker ps`\n",
    "    * get amazon provider version: `docker exec -it <container_id of airflow_scheduler> bash`\n",
    "* Get into the airflow documentation, select the particular version form top right corner, click on python API option and look for `aws.sensors.s3`\n",
    "* you will get the exact library to be imported.\n",
    "* Create an S3 Connection in Airflow:\n",
    "    * Access the Airflow web UI.\n",
    "    * Navigate to Admin -> Connections.\n",
    "    * Click Create.\n",
    "    * Fill in the details:\n",
    "        * Connection Id: Give it a meaningful name, e.g., s3_conn\n",
    "        * Connection Type: Select `Amazon Web Services`\n",
    "        * Login: `<minio loggin id>`\n",
    "        * Password: `<minio loggin password>`\n",
    "        * Extra: `{\"host\": \"http://host.docker.internal:9000\", \"region_name\": null, \"endpoint_url\": \"http://host.docker.internal:9000\", \"use_ssl\": false}`\n",
    "    >> NOTE: Testing the connection will fail in Airflow, but using connection using the dag will create a connection.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow Hooks S3 PostgreSQL\n",
    "\n",
    "* To read data from DB and stoing it into the S3 bucker.\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
